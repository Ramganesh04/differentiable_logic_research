{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37f466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def get_unique_connections(in_dim, out_dim, device='cuda'):\n",
    "    assert out_dim * 2 >= in_dim, 'The number of neurons ({}) must not be smaller than half of the number of inputs ' \\\n",
    "                                  '({}) because otherwise not all inputs could be used or considered.'.format(\n",
    "        out_dim, in_dim\n",
    "    )\n",
    "\n",
    "    x = torch.arange(in_dim).long().unsqueeze(0)\n",
    "\n",
    "    # Take pairs (0, 1), (2, 3), (4, 5), ...\n",
    "    a, b = x[..., ::2], x[..., 1::2]\n",
    "    if a.shape[-1] != b.shape[-1]:\n",
    "        m = min(a.shape[-1], b.shape[-1])\n",
    "        a = a[..., :m]\n",
    "        b = b[..., :m]\n",
    "\n",
    "    # If this was not enough, take pairs (1, 2), (3, 4), (5, 6), ...\n",
    "    if a.shape[-1] < out_dim:\n",
    "        a_, b_ = x[..., 1::2], x[..., 2::2]\n",
    "        a = torch.cat([a, a_], dim=-1)\n",
    "        b = torch.cat([b, b_], dim=-1)\n",
    "        if a.shape[-1] != b.shape[-1]:\n",
    "            m = min(a.shape[-1], b.shape[-1])\n",
    "            a = a[..., :m]\n",
    "            b = b[..., :m]\n",
    "\n",
    "    # If this was not enough, take pairs with offsets >= 2:\n",
    "    offset = 2\n",
    "    while out_dim > a.shape[-1] > offset:\n",
    "        a_, b_ = x[..., :-offset], x[..., offset:]\n",
    "        a = torch.cat([a, a_], dim=-1)\n",
    "        b = torch.cat([b, b_], dim=-1)\n",
    "        offset += 1\n",
    "        assert a.shape[-1] == b.shape[-1], (a.shape[-1], b.shape[-1])\n",
    "\n",
    "    if a.shape[-1] >= out_dim:\n",
    "        a = a[..., :out_dim]\n",
    "        b = b[..., :out_dim]\n",
    "    else:\n",
    "        assert False, (a.shape[-1], offset, out_dim)\n",
    "\n",
    "    perm = torch.randperm(out_dim)\n",
    "\n",
    "    a = a[:, perm].squeeze(0)\n",
    "    b = b[:, perm].squeeze(0)\n",
    "\n",
    "    a, b = a.to(torch.int64), b.to(torch.int64)\n",
    "    a, b = a.to(device), b.to(device)\n",
    "    a, b = a.contiguous(), b.contiguous()\n",
    "    return a, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9278a165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 0, 2], device='cuda:0'), tensor([2, 1, 3], device='cuda:0'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_unique_connections(4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5ab5b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COMPARISON: LogSumExp vs Softmax in PyTorch\n",
      "============================================================\n",
      "\n",
      "Input logits (16 values):\n",
      "tensor([ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784, -1.2345, -0.0431, -1.6047,\n",
      "        -0.7521,  1.6487, -0.3925, -1.4036, -0.7279, -0.5594, -0.7688,  0.7624],\n",
      "       requires_grad=True)\n",
      "Shape: torch.Size([16])\n",
      "\n",
      "============================================================\n",
      "1. BASIC COMPUTATIONS\n",
      "============================================================\n",
      "\n",
      "LogSumExp (scalar): 3.316051\n",
      "\n",
      "Softmax (distribution):\n",
      "tensor([0.2493, 0.1606, 0.0893, 0.0044, 0.0715, 0.0106, 0.0348, 0.0073, 0.0171,\n",
      "        0.1888, 0.0245, 0.0089, 0.0175, 0.0207, 0.0168, 0.0778],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Sum of softmax: 1.000000\n",
      "\n",
      "============================================================\n",
      "2. MATHEMATICAL RELATIONSHIP\n",
      "============================================================\n",
      "\n",
      "Softmax using F.softmax:\n",
      "tensor([0.2493, 0.1606, 0.0893, 0.0044, 0.0715, 0.0106, 0.0348, 0.0073, 0.0171,\n",
      "        0.1888, 0.0245, 0.0089, 0.0175, 0.0207, 0.0168, 0.0778],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "Softmax using exp(x - logsumexp(x)):\n",
      "tensor([0.2493, 0.1606, 0.0893, 0.0044, 0.0715, 0.0106, 0.0348, 0.0073, 0.0171,\n",
      "        0.1888, 0.0245, 0.0089, 0.0175, 0.0207, 0.0168, 0.0778],\n",
      "       grad_fn=<ExpBackward0>)\n",
      "\n",
      "Are they equal? True\n",
      "Max difference: 2.98e-08\n",
      "\n",
      "============================================================\n",
      "3. NUMERICAL STABILITY TEST\n",
      "============================================================\n",
      "\n",
      "Extreme logits:\n",
      "tensor([ 1000.,   999.,   998.,   997., -1000.,  -999.,  -998.,  -997.,   500.,\n",
      "            0.,  -500.,   100.,  -100.,    50.,   -50.,     1.],\n",
      "       requires_grad=True)\n",
      "\n",
      "Logsumexp (stable): 1000.440186\n",
      "\n",
      "Softmax (stable):\n",
      "tensor([0.6439, 0.2369, 0.0871, 0.0321, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "No NaN/Inf: True\n",
      "\n",
      "Manual exp/sum (unstable):\n",
      "tensor([nan, nan, nan, nan, 0., 0., 0., 0., nan, 0., 0., nan, 0., 0., 0., 0.],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Contains Inf? True\n",
      "\n",
      "============================================================\n",
      "4. LOG-SOFTMAX (RELATED OPERATION)\n",
      "============================================================\n",
      "\n",
      "Log-Softmax using F.log_softmax:\n",
      "tensor([-1.3891, -1.8288, -2.4153, -5.4216, -2.6376, -4.5506, -3.3591, -4.9207,\n",
      "        -4.0682, -1.6673, -3.7085, -4.7197, -4.0439, -3.8755, -4.0849, -2.5536],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "\n",
      "Log-Softmax using x - logsumexp(x):\n",
      "tensor([-1.3891, -1.8288, -2.4153, -5.4216, -2.6376, -4.5506, -3.3591, -4.9207,\n",
      "        -4.0682, -1.6673, -3.7085, -4.7197, -4.0439, -3.8755, -4.0849, -2.5536],\n",
      "       grad_fn=<SubBackward0>)\n",
      "\n",
      "Are they equal? True\n",
      "\n",
      "============================================================\n",
      "5. PERFORMANCE COMPARISON\n",
      "============================================================\n",
      "\n",
      "Timing (10000 iterations, 10000 elements):\n",
      "LogSumExp:   0.1375s (13.75 µs/iter)\n",
      "Softmax:     0.0752s (7.52 µs/iter)\n",
      "Log-Softmax: 0.0744s (7.44 µs/iter)\n",
      "\n",
      "============================================================\n",
      "6. GRADIENT COMPUTATION\n",
      "============================================================\n",
      "\n",
      "Gradient of LogSumExp:\n",
      "tensor([0.1031, 0.0168, 0.0213, 0.2453, 0.0231, 0.0183, 0.0547, 0.0038, 0.0456,\n",
      "        0.0323, 0.0194, 0.0240, 0.0774, 0.0745, 0.2368, 0.0035])\n",
      "Sum of gradients: 1.000000\n",
      "(Note: LSE gradient equals softmax)\n",
      "\n",
      "Softmax of same input:\n",
      "tensor([0.1031, 0.0168, 0.0213, 0.2453, 0.0231, 0.0183, 0.0547, 0.0038, 0.0456,\n",
      "        0.0323, 0.0194, 0.0240, 0.0774, 0.0745, 0.2368, 0.0035])\n",
      "Gradients match softmax? True\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "# Create 16 random logits\n",
    "torch.manual_seed(42)\n",
    "logits = torch.randn(16, requires_grad=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPARISON: LogSumExp vs Softmax in PyTorch\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nInput logits (16 values):\\n{logits}\")\n",
    "print(f\"Shape: {logits.shape}\")\n",
    "\n",
    "# ============================================================\n",
    "# 1. BASIC COMPUTATIONS\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"1. BASIC COMPUTATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# LogSumExp: returns a scalar (log of sum of exponentials)\n",
    "lse = torch.logsumexp(logits, dim=0)\n",
    "print(f\"\\nLogSumExp (scalar): {lse.item():.6f}\")\n",
    "\n",
    "# Softmax: returns a probability distribution (same shape as input)\n",
    "softmax_out = F.softmax(logits, dim=0)\n",
    "print(f\"\\nSoftmax (distribution):\\n{softmax_out}\")\n",
    "print(f\"Sum of softmax: {softmax_out.sum().item():.6f}\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. MATHEMATICAL RELATIONSHIP\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2. MATHEMATICAL RELATIONSHIP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Softmax is defined as: exp(x_i) / sum(exp(x_j))\n",
    "# Which equals: exp(x_i - logsumexp(x))\n",
    "\n",
    "# Method 1: Using softmax function\n",
    "softmax_v1 = F.softmax(logits, dim=0)\n",
    "\n",
    "# Method 2: Manual calculation using logsumexp\n",
    "lse = torch.logsumexp(logits, dim=0)\n",
    "softmax_v2 = torch.exp(logits - lse)\n",
    "\n",
    "print(f\"\\nSoftmax using F.softmax:\\n{softmax_v1}\")\n",
    "print(f\"\\nSoftmax using exp(x - logsumexp(x)):\\n{softmax_v2}\")\n",
    "print(f\"\\nAre they equal? {torch.allclose(softmax_v1, softmax_v2)}\")\n",
    "print(f\"Max difference: {(softmax_v1 - softmax_v2).abs().max().item():.2e}\")\n",
    "\n",
    "# ============================================================\n",
    "# 3. NUMERICAL STABILITY TEST\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"3. NUMERICAL STABILITY TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test with extreme values\n",
    "extreme_logits = torch.tensor([1000.0, 999.0, 998.0, 997.0, -1000.0, -999.0, \n",
    "                               -998.0, -997.0, 500.0, 0.0, -500.0, 100.0, \n",
    "                               -100.0, 50.0, -50.0, 1.0], requires_grad=True)\n",
    "\n",
    "print(f\"\\nExtreme logits:\\n{extreme_logits}\")\n",
    "\n",
    "# Safe: Using logsumexp (numerically stable)\n",
    "lse_stable = torch.logsumexp(extreme_logits, dim=0)\n",
    "print(f\"\\nLogsumexp (stable): {lse_stable.item():.6f}\")\n",
    "\n",
    "# Safe: Using softmax (uses logsumexp internally)\n",
    "softmax_stable = F.softmax(extreme_logits, dim=0)\n",
    "print(f\"\\nSoftmax (stable):\\n{softmax_stable}\")\n",
    "print(f\"No NaN/Inf: {not torch.isnan(softmax_stable).any() and not torch.isinf(softmax_stable).any()}\")\n",
    "\n",
    "# Unsafe: Manual exp then sum (can overflow/underflow)\n",
    "try:\n",
    "    manual_exp = torch.exp(extreme_logits)\n",
    "    manual_sum = manual_exp.sum()\n",
    "    manual_softmax = manual_exp / manual_sum\n",
    "    print(f\"\\nManual exp/sum (unstable):\\n{manual_softmax}\")\n",
    "    print(f\"Contains Inf? {torch.isinf(manual_exp).any()}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nManual calculation failed: {e}\")\n",
    "\n",
    "# ============================================================\n",
    "# 4. LOG-SOFTMAX COMPARISON\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"4. LOG-SOFTMAX (RELATED OPERATION)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# LogSoftmax = log(softmax(x)) = x - logsumexp(x)\n",
    "log_softmax_v1 = F.log_softmax(logits, dim=0)\n",
    "log_softmax_v2 = logits - torch.logsumexp(logits, dim=0)\n",
    "\n",
    "print(f\"\\nLog-Softmax using F.log_softmax:\\n{log_softmax_v1}\")\n",
    "print(f\"\\nLog-Softmax using x - logsumexp(x):\\n{log_softmax_v2}\")\n",
    "print(f\"\\nAre they equal? {torch.allclose(log_softmax_v1, log_softmax_v2)}\")\n",
    "\n",
    "# ============================================================\n",
    "# 5. PERFORMANCE COMPARISON\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"5. PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create larger tensor for timing\n",
    "large_logits = torch.randn(10000, requires_grad=True)\n",
    "n_iterations = 10000\n",
    "\n",
    "# Time logsumexp\n",
    "start = time.time()\n",
    "for _ in range(n_iterations):\n",
    "    _ = torch.logsumexp(large_logits, dim=0)\n",
    "lse_time = time.time() - start\n",
    "\n",
    "# Time softmax\n",
    "start = time.time()\n",
    "for _ in range(n_iterations):\n",
    "    _ = F.softmax(large_logits, dim=0)\n",
    "softmax_time = time.time() - start\n",
    "\n",
    "# Time log_softmax\n",
    "start = time.time()\n",
    "for _ in range(n_iterations):\n",
    "    _ = F.log_softmax(large_logits, dim=0)\n",
    "log_softmax_time = time.time() - start\n",
    "\n",
    "print(f\"\\nTiming (10000 iterations, 10000 elements):\")\n",
    "print(f\"LogSumExp:   {lse_time:.4f}s ({lse_time/n_iterations*1e6:.2f} µs/iter)\")\n",
    "print(f\"Softmax:     {softmax_time:.4f}s ({softmax_time/n_iterations*1e6:.2f} µs/iter)\")\n",
    "print(f\"Log-Softmax: {log_softmax_time:.4f}s ({log_softmax_time/n_iterations*1e6:.2f} µs/iter)\")\n",
    "\n",
    "# ============================================================\n",
    "# 6. GRADIENT COMPUTATION\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"6. GRADIENT COMPUTATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "logits_grad = torch.randn(16, requires_grad=True)\n",
    "\n",
    "# Gradient of logsumexp\n",
    "lse_out = torch.logsumexp(logits_grad, dim=0)\n",
    "lse_out.backward()\n",
    "print(f\"\\nGradient of LogSumExp:\")\n",
    "print(f\"{logits_grad.grad}\")\n",
    "print(f\"Sum of gradients: {logits_grad.grad.sum().item():.6f}\")\n",
    "print(f\"(Note: LSE gradient equals softmax)\")\n",
    "\n",
    "# Verify: gradient of logsumexp should equal softmax\n",
    "softmax_check = F.softmax(logits_grad.detach(), dim=0)\n",
    "print(f\"\\nSoftmax of same input:\\n{softmax_check}\")\n",
    "print(f\"Gradients match softmax? {torch.allclose(logits_grad.grad, softmax_check)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22e87801",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.nn.parameter.Parameter(torch.randn(3, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90a64399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1453,  0.8568, -1.0604, -0.4739,  0.8355,  0.5523,  0.9878,  0.2472,\n",
       "         -1.3898,  0.7879,  0.2619,  0.3239, -0.0066, -0.6608,  1.0100,  1.5781],\n",
       "        [ 0.3673,  0.9278,  0.3392, -0.2632, -0.8080,  0.0591, -0.4664, -0.8863,\n",
       "         -0.3549, -0.2807,  0.1187, -1.6083, -0.0530, -0.7546,  1.1250,  0.2149],\n",
       "        [ 1.1615,  1.4790, -0.2460,  0.7622, -0.1894,  0.0621,  0.2685,  0.6053,\n",
       "          0.0126, -0.1968,  1.6451,  1.3394,  1.2698, -0.4576,  0.6375,  0.3438]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5a569e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0411, 0.0000, 0.0000, 0.0339, 0.0000, 0.1004, 0.0000, 0.0000,\n",
       "         0.0204, 0.0000, 0.0000, 0.0000, 0.0000, 0.1130, 0.6912],\n",
       "        [0.0117, 0.3687, 0.0069, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6127, 0.0000],\n",
       "        [0.0641, 0.2564, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.4082, 0.1560, 0.1152, 0.0000, 0.0000, 0.0000]],\n",
       "       grad_fn=<NormmaxBisectFunctionBackward>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from entmax import normmax_bisect\n",
    "\n",
    "normmax_bisect(weights,alpha=1.5,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "000a906b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33689/4122026785.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  torch.nn.functional.softmax(weights)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0327, 0.0891, 0.0131, 0.0236, 0.0872, 0.0657, 0.1016, 0.0484, 0.0094,\n",
       "         0.0832, 0.0492, 0.0523, 0.0376, 0.0195, 0.1039, 0.1834],\n",
       "        [0.0839, 0.1469, 0.0816, 0.0447, 0.0259, 0.0616, 0.0364, 0.0239, 0.0407,\n",
       "         0.0439, 0.0654, 0.0116, 0.0551, 0.0273, 0.1790, 0.0720],\n",
       "        [0.0943, 0.1296, 0.0231, 0.0633, 0.0244, 0.0314, 0.0386, 0.0541, 0.0299,\n",
       "         0.0243, 0.1530, 0.1127, 0.1051, 0.0187, 0.0559, 0.0416]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.softmax(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded788dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
