{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3479b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Hyperparameters (aligned with attached CIFAR notebook) ====\n",
    "BATCH_SIZE       = 128\n",
    "GATE_OPTIMIZER   = 'softmax'   # 'softmax' | 'gumbel_softmax' | 'sparsemax'\n",
    "NETWORK_LR       = 0.01\n",
    "GUMBEL_TAU       = 1\n",
    "GROUP_SUM_TAU    = 100\n",
    "NETWORK_LAYERS   = 8\n",
    "GATES            = 128_000\n",
    "EPOCHS           = 2000\n",
    "NOISE_TEMP = 0.1\n",
    "# Artifacts\n",
    "LOG_CSV          = \"cifar_run_log_new.csv\"           # append-only\n",
    "FINAL_STATS_JSON = \"cifar_final_stats.json\"\n",
    "PLOT_PNG         = \"cifar_acc_discrete_vs_eval.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "451b2714",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ==== Imports ====\n",
    "import os, csv, json, time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from difflogic import LogicLayer, GroupSum\n",
    "from difflogic.packbitstensor import PackBitsTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e39d902",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Device ====\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d76578ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Note: The attached notebook uses CIFAR-10 with Normalize(0.5, 0.5, 0.5), keep the same to avoid deviating. \n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)),\n",
    "])\n",
    "\n",
    "train_ds = datasets.CIFAR10(root='./data', train=True,  download=True, transform=transform)\n",
    "test_ds  = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4a1c327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Model: 3x32x32 -> 3072 input, LogicLayer stack, GroupSum(10) ====\n",
    "def build_model():\n",
    "    layers = [\n",
    "        nn.Flatten(),\n",
    "        LogicLayer(3072, GATES, device='cuda', implementation='cuda',\n",
    "                   gate_function=GATE_OPTIMIZER, gumbel_tau=GUMBEL_TAU)\n",
    "    ]\n",
    "    for _ in range(NETWORK_LAYERS - 1):\n",
    "        layers.append(\n",
    "            LogicLayer(GATES, GATES, device='cuda', implementation='cuda',\n",
    "                       gate_function=GATE_OPTIMIZER, gumbel_tau=GUMBEL_TAU)\n",
    "        )\n",
    "    layers.append(GroupSum(10, tau=GROUP_SUM_TAU))\n",
    "    return nn.Sequential(*layers).to(device)\n",
    "\n",
    "model = build_model()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=NETWORK_LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18180d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_accuracy_float(model, loader, mode='eval'):\n",
    "    orig = model.training\n",
    "    model.train(mode == 'train')\n",
    "    total, correct = 0, 0\n",
    "    for x, y in loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += x.size(0)\n",
    "    model.train(orig)\n",
    "    return correct / max(1, total)\n",
    "\n",
    "def packbits_eval(model, loader):\n",
    "    orig_mode = model.training\n",
    "    start = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        res = np.mean([\n",
    "            (\n",
    "                model(\n",
    "                    PackBitsTensor(\n",
    "                        x.to('cuda').reshape(x.shape[0], -1).round().bool()\n",
    "                    )\n",
    "                ).argmax(-1) == y.to('cuda')\n",
    "            ).to(torch.float32).mean().item()\n",
    "            for x, y in loader\n",
    "        ])\n",
    "    model.train(mode=orig_mode)\n",
    "    elapsed = time.perf_counter() - start\n",
    "    throughput = len(loader.dataset) / elapsed\n",
    "    print(f\"throughput : {throughput:.1f}/s\")\n",
    "    return float(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3525c2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Append-safe CSV header ====\n",
    "fieldnames = [\n",
    "    \"epoch\", \"train_loss\", \"train_acc\",\n",
    "    \"float_eval_acc\", \"float_trainmode_acc\", \"discrete_acc\",\n",
    "    \"BATCH_SIZE\", \"GATE_OPTIMIZER\", \"NETWORK_LR\", \"GUMBEL_TAU\",\n",
    "    \"GROUP_SUM_TAU\", \"NETWORK_LAYERS\", \"GATES\", \"EPOCHS\", \"CONNECTIONS\", \"NOISE_TEMP\"\n",
    "]\n",
    "header_needed = (not os.path.exists(LOG_CSV)) or (os.path.getsize(LOG_CSV) == 0)\n",
    "with open(LOG_CSV, \"a\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    if header_needed:\n",
    "        writer.writeheader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea668a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_gate_optimizer(model, new_gate_optimizer):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, LogicLayer):\n",
    "            module.gate_function = new_gate_optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d333e119",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_opt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m GATE_OPTIMIZER \u001b[38;5;241m=\u001b[39m \u001b[43mnew_opt\u001b[49m\n\u001b[1;32m      2\u001b[0m change_gate_optimizer(model, GATE_OPTIMIZER)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_opt' is not defined"
     ]
    }
   ],
   "source": [
    "GATE_OPTIMIZER = \"sp\"\n",
    "change_gate_optimizer(model, GATE_OPTIMIZER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775a3832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss=2.2751 train_acc=0.1337 float_eval_acc=0.1573 float_train_acc=0.2316 discrete_acc=0.1573\n",
      "Epoch 2: train_loss=2.0051 train_acc=0.2858 float_eval_acc=0.1600 float_train_acc=0.3261 discrete_acc=0.1600\n",
      "Epoch 3: train_loss=1.8439 train_acc=0.3515 float_eval_acc=0.1813 float_train_acc=0.3719 discrete_acc=0.1813\n",
      "Epoch 4: train_loss=1.7475 train_acc=0.3913 float_eval_acc=0.1852 float_train_acc=0.3931 discrete_acc=0.1852\n",
      "Epoch 5: train_loss=1.6810 train_acc=0.4174 float_eval_acc=0.1830 float_train_acc=0.4061 discrete_acc=0.1830\n",
      "Epoch 6: train_loss=1.6326 train_acc=0.4337 float_eval_acc=0.1928 float_train_acc=0.4191 discrete_acc=0.1928\n",
      "Epoch 7: train_loss=1.5908 train_acc=0.4522 float_eval_acc=0.2034 float_train_acc=0.4364 discrete_acc=0.2034\n",
      "Epoch 8: train_loss=1.5544 train_acc=0.4654 float_eval_acc=0.2039 float_train_acc=0.4445 discrete_acc=0.2039\n",
      "Epoch 9: train_loss=1.5224 train_acc=0.4784 float_eval_acc=0.1995 float_train_acc=0.4485 discrete_acc=0.1995\n",
      "Epoch 10: train_loss=1.4900 train_acc=0.4896 float_eval_acc=0.2035 float_train_acc=0.4528 discrete_acc=0.2035\n",
      "Epoch 11: train_loss=1.4698 train_acc=0.4979 float_eval_acc=0.2021 float_train_acc=0.4561 discrete_acc=0.2021\n",
      "Epoch 12: train_loss=1.4405 train_acc=0.5106 float_eval_acc=0.2116 float_train_acc=0.4596 discrete_acc=0.2116\n",
      "Epoch 13: train_loss=1.4283 train_acc=0.5174 float_eval_acc=0.2079 float_train_acc=0.4567 discrete_acc=0.2079\n",
      "Epoch 14: train_loss=1.3840 train_acc=0.5277 float_eval_acc=0.2126 float_train_acc=0.4729 discrete_acc=0.2126\n",
      "Epoch 15: train_loss=1.3640 train_acc=0.5356 float_eval_acc=0.2201 float_train_acc=0.4783 discrete_acc=0.2201\n",
      "Epoch 16: train_loss=1.3293 train_acc=0.5466 float_eval_acc=0.2163 float_train_acc=0.4767 discrete_acc=0.2163\n",
      "Epoch 17: train_loss=1.3710 train_acc=0.5527 float_eval_acc=0.2083 float_train_acc=0.4768 discrete_acc=0.2083\n",
      "Epoch 18: train_loss=1.2957 train_acc=0.5600 float_eval_acc=0.2193 float_train_acc=0.4846 discrete_acc=0.2193\n",
      "Epoch 19: train_loss=1.3312 train_acc=0.5681 float_eval_acc=0.2242 float_train_acc=0.4866 discrete_acc=0.2242\n",
      "Epoch 20: train_loss=1.3596 train_acc=0.5705 float_eval_acc=0.2177 float_train_acc=0.4817 discrete_acc=0.2177\n",
      "Epoch 21: train_loss=1.2289 train_acc=0.5827 float_eval_acc=0.2106 float_train_acc=0.4916 discrete_acc=0.2106\n",
      "Epoch 22: train_loss=1.2014 train_acc=0.5892 float_eval_acc=0.2159 float_train_acc=0.4870 discrete_acc=0.2159\n",
      "Epoch 23: train_loss=1.2253 train_acc=0.5945 float_eval_acc=0.2155 float_train_acc=0.4881 discrete_acc=0.2155\n",
      "Epoch 24: train_loss=1.4356 train_acc=0.5952 float_eval_acc=0.2194 float_train_acc=0.4899 discrete_acc=0.2194\n",
      "Epoch 25: train_loss=1.7810 train_acc=0.5924 float_eval_acc=0.2100 float_train_acc=0.4907 discrete_acc=0.2100\n",
      "Epoch 26: train_loss=1.2570 train_acc=0.6041 float_eval_acc=0.2179 float_train_acc=0.4855 discrete_acc=0.2179\n"
     ]
    }
   ],
   "source": [
    "# ==== Training loop with logging and evaluations ====\n",
    "eval_acc_hist = []\n",
    "disc_acc_hist = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    seen = 0\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        correct += (logits.argmax(1) == y).sum().item()\n",
    "        seen += x.size(0)\n",
    "\n",
    "    train_loss = running_loss / max(1, seen)\n",
    "    train_acc  = correct / max(1, seen)\n",
    "\n",
    "    # Float eval vs train-mode\n",
    "    float_eval_acc      = eval_accuracy_float(model, test_loader, mode='eval')\n",
    "    float_trainmode_acc = eval_accuracy_float(model, test_loader, mode='train')\n",
    "\n",
    "    # Discrete-style evaluation (PackBitsTensor)\n",
    "    discrete_acc = float_eval_acc\n",
    "\n",
    "    print(f\"Epoch {epoch}: \"\n",
    "          f\"train_loss={train_loss:.4f} train_acc={train_acc:.4f} \"\n",
    "          f\"float_eval_acc={float_eval_acc:.4f} float_train_acc={float_trainmode_acc:.4f} \"\n",
    "          f\"discrete_acc={discrete_acc:.4f}\")\n",
    "\n",
    "    eval_acc_hist.append(float_eval_acc)\n",
    "    disc_acc_hist.append(discrete_acc)\n",
    "\n",
    "    # Append log row\n",
    "    with open(LOG_CSV, \"a\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writerow(dict(\n",
    "            epoch=epoch, train_loss=train_loss, train_acc=train_acc,\n",
    "            float_eval_acc=float_eval_acc, float_trainmode_acc=float_trainmode_acc,\n",
    "            discrete_acc=discrete_acc, \n",
    "            BATCH_SIZE=BATCH_SIZE, GATE_OPTIMIZER=GATE_OPTIMIZER, NETWORK_LR=NETWORK_LR,\n",
    "            GUMBEL_TAU=GUMBEL_TAU, GROUP_SUM_TAU=GROUP_SUM_TAU,\n",
    "            NETWORK_LAYERS=NETWORK_LAYERS, GATES=GATES, EPOCHS=EPOCHS\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063298a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Plot: discrete vs float-eval accuracy ====\n",
    "plt.figure(figsize=(7, 4))\n",
    "steps = np.arange(1, len(eval_acc_hist) + 1)\n",
    "plt.plot(steps, eval_acc_hist, marker='o', linewidth=1.8, markersize=4, label='float eval')\n",
    "plt.plot(steps, disc_acc_hist, marker='s', linewidth=1.8, markersize=4, label='discrete (PackBits)')\n",
    "plt.title('CIFAR-10 Accuracy per epoch (discrete vs eval)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True, linestyle='--', alpha=0.4)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOT_PNG, dpi=150)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51319608",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ==== Data: CIFAR-10 (as in the notebook) ====\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ==== Save final stats ====\n",
    "final_stats = dict(\n",
    "    final_epoch=EPOCHS,\n",
    "    final_train_loss=train_loss,\n",
    "    final_train_acc=train_acc,\n",
    "    final_float_eval_acc=eval_acc_hist[-1],\n",
    "    final_discrete_acc=disc_acc_hist[-1],\n",
    "    hyperparams=dict(\n",
    "        BATCH_SIZE=BATCH_SIZE,\n",
    "        GATE_OPTIMIZER=GATE_OPTIMIZER,\n",
    "        NETWORK_LR=NETWORK_LR,\n",
    "        GUMBEL_TAU=GUMBEL_TAU,\n",
    "        GROUP_SUM_TAU=GROUP_SUM_TAU,\n",
    "        NETWORK_LAYERS=NETWORK_LAYERS,\n",
    "        GATES=GATES,\n",
    "        EPOCHS=EPOCHS,\n",
    "    ),\n",
    "    log_csv=LOG_CSV,\n",
    "    acc_plot=PLOT_PNG,\n",
    ")\n",
    "with open(FINAL_STATS_JSON, \"w\") as f:\n",
    "    json.dump(final_stats, f, indent=2)\n",
    "print(f\"Saved final stats to {FINAL_STATS_JSON}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
