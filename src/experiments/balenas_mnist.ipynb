{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f40553b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GATE_OPTIMIZER = 'balenas'  # CHANGED TO BALENAS\n",
    "NETWORK_LR = 3e-4  # Lower LR for mean\n",
    "VARIANCE_LR = 1e-3  # Higher LR for variance\n",
    "GUMBEL_TAU = 0.25\n",
    "GROUP_SUM_TAU = 30\n",
    "NETWORK_LAYERS = 2\n",
    "GATES = 900\n",
    "EPOCHS = 20\n",
    "ENTMAX_ALPHA = 1.5\n",
    "CONNECTIONS = 'random'\n",
    "KL_BETA = 1e-10  # KL regularization weight\n",
    "LOG_CSV          = \"../logs/run_log_BALENAS_connections.csv\"\n",
    "FINAL_STATS_JSON = \"final_stats.json\"\n",
    "PLOT_PNG         = \"acc_discrete_vs_eval.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c1917253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Imports ====\n",
    "import os, csv, json, time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from difflogic import LogicLayer, GroupSum\n",
    "from difflogic.packbitstensor import PackBitsTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "16fbc3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "e93f8517",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "train_ds = datasets.MNIST(root='../data', train=True, download=True, transform=transform)\n",
    "test_ds = datasets.MNIST(root='../data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8ff113fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    layers = [nn.Flatten(),\n",
    "              LogicLayer(784, GATES, device='cuda', implementation='cuda',\n",
    "                        gate_function=GATE_OPTIMIZER, \n",
    "                        gumbel_tau=GUMBEL_TAU, \n",
    "                        connections=CONNECTIONS,\n",
    "                        entmax_alpha=ENTMAX_ALPHA)]\n",
    "    for _ in range(NETWORK_LAYERS - 1):\n",
    "        layers.append(\n",
    "            LogicLayer(GATES, GATES, device='cuda', implementation='cuda',\n",
    "                      gate_function=GATE_OPTIMIZER, \n",
    "                      gumbel_tau=GUMBEL_TAU, \n",
    "                      connections=CONNECTIONS,\n",
    "                      entmax_alpha=ENTMAX_ALPHA)\n",
    "        )\n",
    "    layers.append(GroupSum(10, tau=GROUP_SUM_TAU))\n",
    "    return nn.Sequential(*layers).to(device)\n",
    "\n",
    "model = build_model()\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7b2ae973",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_params = [p for n, p in model.named_parameters() if 'logvar' not in n]\n",
    "variance_params = [p for n, p in model.named_parameters() if 'logvar' in n]\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': mean_params, 'lr': NETWORK_LR},\n",
    "    {'params': variance_params, 'lr': VARIANCE_LR}\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "efcb72de",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_accuracy_float(model, loader, mode='eval'):\n",
    "    \"\"\"Evaluate with float inputs; compare model.train() vs model.eval() as requested.\"\"\"\n",
    "    orig = model.training\n",
    "    if mode == 'train':\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    total, correct = 0, 0\n",
    "    for x, y in loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += x.size(0)\n",
    "    model.train(orig)\n",
    "    return correct / max(1, total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c7858cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def packbits_eval(model, loader):\n",
    "    \"\"\"Discrete-style eval using PackBitsTensor (as in notebook).\"\"\"\n",
    "    orig_mode = model.training\n",
    "    start = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        res = np.mean([\n",
    "            (\n",
    "                model(\n",
    "                    PackBitsTensor(\n",
    "                        x.to('cuda').reshape(x.shape[0], -1).round().bool()\n",
    "                    )\n",
    "                ).argmax(-1) == y.to('cuda')\n",
    "            ).to(torch.float32).mean().item()\n",
    "            for x, y in loader\n",
    "        ])\n",
    "    model.train(mode=orig_mode)\n",
    "    elapsed = time.perf_counter() - start\n",
    "    throughput = 10_000 / elapsed\n",
    "    print(f\"throughput : {throughput:.1f}/s\")\n",
    "    return float(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e6a4fe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "fieldnames = [\n",
    "    \"epoch\", \"train_loss\", \"train_acc\", \"kl_loss\",\n",
    "    \"float_eval_acc\", \"float_trainmode_acc\", \"discrete_acc\",\n",
    "    \"BATCH_SIZE\", \"GATE_OPTIMIZER\", \"NETWORK_LR\", \"VARIANCE_LR\", \"KL_BETA\",\n",
    "    \"GUMBEL_TAU\", \"GROUP_SUM_TAU\", \"NETWORK_LAYERS\", \"GATES\", \"EPOCHS\", \"CONNECTIONS\"\n",
    "]\n",
    "header_needed = (not os.path.exists(LOG_CSV)) or (os.path.getsize(LOG_CSV) == 0)\n",
    "with open(LOG_CSV, \"a\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    if header_needed:\n",
    "        writer.writeheader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "4c88b62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss=2.2987 train_acc=0.1155 kl_loss=144216.0907 float_train_acc=0.1851 discrete_acc=0.1155\n",
      "Epoch 2: train_loss=2.2909 train_acc=0.1982 kl_loss=144422.1060 float_train_acc=0.1889 discrete_acc=0.1828\n",
      "Epoch 3: train_loss=2.2803 train_acc=0.1919 kl_loss=144859.1979 float_train_acc=0.2000 discrete_acc=0.2521\n",
      "Epoch 4: train_loss=2.2660 train_acc=0.2168 kl_loss=145542.2660 float_train_acc=0.2450 discrete_acc=0.3251\n",
      "Epoch 5: train_loss=2.2480 train_acc=0.2711 kl_loss=146473.2450 float_train_acc=0.3112 discrete_acc=0.3882\n",
      "Epoch 6: train_loss=2.2270 train_acc=0.3339 kl_loss=147646.5529 float_train_acc=0.3902 discrete_acc=0.4673\n",
      "Epoch 7: train_loss=2.2038 train_acc=0.4097 kl_loss=149035.7175 float_train_acc=0.4556 discrete_acc=0.5136\n",
      "Epoch 8: train_loss=2.1793 train_acc=0.4619 kl_loss=150631.9480 float_train_acc=0.5006 discrete_acc=0.5472\n",
      "Epoch 9: train_loss=2.1545 train_acc=0.5042 kl_loss=152401.6394 float_train_acc=0.5351 discrete_acc=0.5586\n",
      "Epoch 10: train_loss=2.1300 train_acc=0.5340 kl_loss=154331.6883 float_train_acc=0.5645 discrete_acc=0.5620\n",
      "Epoch 11: train_loss=2.1064 train_acc=0.5560 kl_loss=156405.0261 float_train_acc=0.5859 discrete_acc=0.5775\n",
      "Epoch 12: train_loss=2.0841 train_acc=0.5733 kl_loss=158597.8150 float_train_acc=0.6037 discrete_acc=0.5600\n",
      "Epoch 13: train_loss=2.0632 train_acc=0.5943 kl_loss=160890.5301 float_train_acc=0.6177 discrete_acc=0.5873\n",
      "Epoch 14: train_loss=2.0437 train_acc=0.6039 kl_loss=163297.4211 float_train_acc=0.6298 discrete_acc=0.6152\n",
      "Epoch 15: train_loss=2.0256 train_acc=0.6168 kl_loss=165792.2800 float_train_acc=0.6416 discrete_acc=0.6288\n",
      "Epoch 16: train_loss=2.0089 train_acc=0.6280 kl_loss=168375.3432 float_train_acc=0.6519 discrete_acc=0.6364\n",
      "Epoch 17: train_loss=1.9936 train_acc=0.6376 kl_loss=171044.1422 float_train_acc=0.6596 discrete_acc=0.6542\n",
      "Epoch 18: train_loss=1.9795 train_acc=0.6451 kl_loss=173799.5659 float_train_acc=0.6676 discrete_acc=0.6491\n",
      "Epoch 19: train_loss=1.9665 train_acc=0.6526 kl_loss=176631.3524 float_train_acc=0.6727 discrete_acc=0.6607\n",
      "Epoch 20: train_loss=1.9547 train_acc=0.6579 kl_loss=179547.2431 float_train_acc=0.6795 discrete_acc=0.6622\n"
     ]
    }
   ],
   "source": [
    "eval_acc_hist = []\n",
    "disc_acc_hist = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_kl = 0.0\n",
    "    correct = 0\n",
    "    seen = 0\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        \n",
    "        # Task loss\n",
    "        task_loss = criterion(logits, y)\n",
    "        \n",
    "        # BaLeNAS KL regularization\n",
    "        kl_loss = sum(\n",
    "            layer.get_kl_loss() \n",
    "            for layer in model.modules() \n",
    "            if isinstance(layer, LogicLayer)\n",
    "        )\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = task_loss + KL_BETA * kl_loss\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += task_loss.item() * x.size(0)\n",
    "        running_kl += kl_loss.item()\n",
    "        correct += (logits.argmax(1) == y).sum().item()\n",
    "        seen += x.size(0)\n",
    "\n",
    "    train_loss = running_loss / max(1, seen)\n",
    "    train_acc = correct / max(1, seen)\n",
    "    avg_kl = running_kl / len(train_loader)\n",
    "\n",
    "    # Float eval in eval-mode vs train-mode\n",
    "    float_eval_acc = eval_accuracy_float(model, test_loader, mode='eval')\n",
    "    float_trainmode_acc = eval_accuracy_float(model, test_loader, mode='train')\n",
    "\n",
    "    # Discrete-style accuracy (PackBits)\n",
    "    discrete_acc = float_eval_acc\n",
    "\n",
    "    # Print concise progress\n",
    "    print(f\"Epoch {epoch}: \"\n",
    "          f\"train_loss={train_loss:.4f} train_acc={train_acc:.4f} \"\n",
    "          f\"kl_loss={avg_kl:.4f} \"\n",
    "          f\"float_train_acc={float_trainmode_acc:.4f} \"\n",
    "          f\"discrete_acc={discrete_acc:.4f}\")\n",
    "\n",
    "    # Track for plotting\n",
    "    eval_acc_hist.append(float_trainmode_acc)\n",
    "    disc_acc_hist.append(float_eval_acc)\n",
    "\n",
    "    # Log row with hyperparams repeated\n",
    "    with open(LOG_CSV, \"a\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writerow(dict(\n",
    "            epoch=epoch, train_loss=train_loss, train_acc=train_acc, kl_loss=avg_kl,\n",
    "            float_eval_acc=float_eval_acc, float_trainmode_acc=float_trainmode_acc,\n",
    "            discrete_acc=discrete_acc,\n",
    "            BATCH_SIZE=BATCH_SIZE, GATE_OPTIMIZER=GATE_OPTIMIZER, \n",
    "            NETWORK_LR=NETWORK_LR, VARIANCE_LR=VARIANCE_LR, KL_BETA=KL_BETA,\n",
    "            GUMBEL_TAU=GUMBEL_TAU, GROUP_SUM_TAU=GROUP_SUM_TAU,\n",
    "            NETWORK_LAYERS=NETWORK_LAYERS, GATES=GATES, EPOCHS=EPOCHS, \n",
    "            CONNECTIONS=CONNECTIONS\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648d594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for module in model.modules():\n",
    "    if isinstance(module, LogicLayer):\n",
    "        module.gate_function = 'softmax'\n",
    "            # You may also need to re-initialize related internal parameters or settings if referenced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63927d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss=0.9871 train_acc=0.8215 float_train_acc=0.8318 discrete_acc=0.8328\n",
      "Epoch 2: train_loss=0.9631 train_acc=0.8241 float_train_acc=0.8339 discrete_acc=0.8331\n",
      "Epoch 3: train_loss=0.9415 train_acc=0.8263 float_train_acc=0.8361 discrete_acc=0.8332\n",
      "Epoch 4: train_loss=0.9220 train_acc=0.8283 float_train_acc=0.8389 discrete_acc=0.8378\n",
      "Epoch 5: train_loss=0.9043 train_acc=0.8305 float_train_acc=0.8406 discrete_acc=0.8393\n",
      "Epoch 6: train_loss=0.8883 train_acc=0.8320 float_train_acc=0.8430 discrete_acc=0.8426\n",
      "Epoch 7: train_loss=0.8737 train_acc=0.8341 float_train_acc=0.8445 discrete_acc=0.8432\n",
      "Epoch 8: train_loss=0.8604 train_acc=0.8354 float_train_acc=0.8455 discrete_acc=0.8444\n",
      "Epoch 9: train_loss=0.8483 train_acc=0.8366 float_train_acc=0.8467 discrete_acc=0.8444\n",
      "Epoch 10: train_loss=0.8372 train_acc=0.8379 float_train_acc=0.8486 discrete_acc=0.8483\n",
      "Epoch 11: train_loss=0.8271 train_acc=0.8398 float_train_acc=0.8496 discrete_acc=0.8507\n",
      "Epoch 12: train_loss=0.8177 train_acc=0.8410 float_train_acc=0.8508 discrete_acc=0.8509\n",
      "Epoch 13: train_loss=0.8091 train_acc=0.8421 float_train_acc=0.8526 discrete_acc=0.8525\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, y)\n\u001b[1;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 18\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     21\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (logits\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m y)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.venv/lib/python3.12/site-packages/torch/optim/adam.py:226\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    214\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    216\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    217\u001b[0m         group,\n\u001b[1;32m    218\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    223\u001b[0m         state_steps,\n\u001b[1;32m    224\u001b[0m     )\n\u001b[0;32m--> 226\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:161\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.12/site-packages/torch/optim/adam.py:734\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;66;03m# Respect when the user inputs False/True for foreach or fused. We only want to change\u001b[39;00m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;66;03m# the default when neither have been user-specified. Note that we default to foreach\u001b[39;00m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;66;03m# and pass False to use_fused. This is not a mistake--we want to give the fused impl\u001b[39;00m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;66;03m# bake-in time before making it the default, even if it is typically faster.\u001b[39;00m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m foreach \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 734\u001b[0m     _, foreach \u001b[38;5;241m=\u001b[39m \u001b[43m_default_to_fused_or_foreach\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;66;03m# Do not flip on foreach for the unsupported case where lr is a Tensor and capturable=False.\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m foreach \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(lr, Tensor) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m capturable:\n",
      "File \u001b[0;32m~/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:180\u001b[0m, in \u001b[0;36m_default_to_fused_or_foreach\u001b[0;34m(params, differentiable, use_fused)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m fused_supported_devices \u001b[38;5;241m=\u001b[39m \u001b[43m_get_fused_kernels_supported_devices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m foreach_supported_devices \u001b[38;5;241m=\u001b[39m _get_foreach_kernels_supported_devices()\n\u001b[1;32m    182\u001b[0m fused \u001b[38;5;241m=\u001b[39m use_fused \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    183\u001b[0m     p \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m params\n\u001b[1;32m    190\u001b[0m )\n",
      "File \u001b[0;32m~/.venv/lib/python3.12/site-packages/torch/utils/_foreach_utils.py:14\u001b[0m, in \u001b[0;36m_get_fused_kernels_supported_devices\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_fused_kernels_supported_devices\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return the device type list that supports fused kernels in optimizer.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_privateuse1_backend_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval_acc_hist = []\n",
    "disc_acc_hist = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    seen = 0\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        correct += (logits.argmax(1) == y).sum().item()\n",
    "        seen += x.size(0)\n",
    "\n",
    "    train_loss = running_loss / max(1, seen)\n",
    "    train_acc  = correct / max(1, seen)\n",
    "\n",
    "    # Float eval in eval-mode vs train-mode\n",
    "    float_eval_acc       = eval_accuracy_float(model, test_loader, mode='eval')\n",
    "    float_trainmode_acc  = eval_accuracy_float(model, test_loader, mode='train')\n",
    "\n",
    "    # Discrete-style accuracy (PackBits)\n",
    "    discrete_acc = float_eval_acc\n",
    "\n",
    "    # Print concise progress\n",
    "    print(f\"Epoch {epoch}: \"\n",
    "          f\"train_loss={train_loss:.4f} train_acc={train_acc:.4f} \"\n",
    "          f\"float_train_acc={float_trainmode_acc:.4f} \"\n",
    "          f\"discrete_acc={discrete_acc:.4f}\")\n",
    "\n",
    "    # Track for plotting\n",
    "    eval_acc_hist.append(float_trainmode_acc)\n",
    "    disc_acc_hist.append(float_eval_acc)\n",
    "\n",
    "    # Log row with hyperparams repeated (simple single-file log)\n",
    "    with open(LOG_CSV, \"a\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writerow(dict(\n",
    "            epoch=epoch, train_loss=train_loss, train_acc=train_acc,\n",
    "            float_eval_acc=float_eval_acc, float_trainmode_acc=float_trainmode_acc,\n",
    "            discrete_acc=discrete_acc,\n",
    "            BATCH_SIZE=BATCH_SIZE, GATE_OPTIMIZER=GATE_OPTIMIZER, NETWORK_LR=NETWORK_LR,\n",
    "            GUMBEL_TAU=GUMBEL_TAU, GROUP_SUM_TAU=GROUP_SUM_TAU,\n",
    "            NETWORK_LAYERS=NETWORK_LAYERS, GATES=GATES, EPOCHS=EPOCHS, \n",
    "            CONNECTIONS=CONNECTIONS\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e81f903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
